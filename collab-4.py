from faster_whisper import WhisperModel
import torch
import datetime
import subprocess
import time
from pyannote.audio import Pipeline
import warnings
import json
warnings.filterwarnings("ignore")

import os

from dotenv import load_dotenv
load_dotenv()

# -------- CONFIG --------
AUDIO_FILE = "audio.m4a"
MODEL_SIZE = "medium"  # tiny, base, small, medium, large-v2, large-v3
LANGUAGE = "en"  # Use language code: en, es, fr, etc.
HF_TOKEN = os.getenv("HF_TOKEN")
OUTPUT_FILE = f"final_transcript_{MODEL_SIZE}.txt"

# Optimization settings
COMPUTE_TYPE = "int8"  # int8 (fastest), float16, float32
BEAM_SIZE = 5  # Lower = faster, 5 is good balance
# -------------------------

def format_time(secs):
    return str(datetime.timedelta(seconds=round(secs)))

start_all = time.time()

# ---------------------------------
# STEP 1: Convert to WAV if needed
# ---------------------------------
t0 = time.time()
wav_file = "audio.wav"

if not AUDIO_FILE.endswith(".wav"):
    print("üîÑ Converting to WAV...")
    subprocess.call([
        "ffmpeg", "-i", AUDIO_FILE,
        "-ar", "16000",  # 16kHz for Whisper
        "-ac", "1",      # Mono
        "-c:a", "pcm_s16le",
        wav_file, "-y", "-loglevel", "quiet"
    ])
    AUDIO_FILE = wav_file
    print(f"‚úÖ Conversion done in {time.time() - t0:.2f}s")

# ---------------------------------
# STEP 2: Load faster-whisper model
# ---------------------------------
t1 = time.time()
print(f"\n‚öôÔ∏è Loading faster-whisper model: {MODEL_SIZE}...")

# faster-whisper uses CPU efficiently on Mac M4
model = WhisperModel(
    MODEL_SIZE,
    device="cpu",  # Use CPU for M4 (optimized)
    compute_type=COMPUTE_TYPE,  # int8 for speed
    num_workers=4  # Parallel processing
)
print(f"‚úÖ Model loaded in {time.time() - t1:.2f}s")

# ---------------------------------
# STEP 3: Fast transcription with timestamps
# ---------------------------------
t2 = time.time()
print("\nüéß Transcribing with faster-whisper...")

segments_list = []
transcription = ''
segments_iter, info = model.transcribe(
    AUDIO_FILE,
    language=LANGUAGE,
    beam_size=BEAM_SIZE,
    vad_filter=True,  # Voice activity detection - removes silence
    vad_parameters=dict(min_silence_duration_ms=100),
    word_timestamps=False,  # Set to True if you need word-level timing
    condition_on_previous_text=True
)

# Convert generator to list and keep timestamps
for segment in segments_iter:
    segments_list.append({
        "start": segment.start,
        "end": segment.end,
        "text": segment.text
    })
    transcription += segment.text

print(transcription)

print(f"‚úÖ Transcription complete in {time.time() - t2:.2f}s")
print(f"üìÑ Total segments: {len(segments_list)}")
print(f"üéµ Detected language: {info.language} (probability: {info.language_probability:.2%})")

# ---------------------------------
# STEP 4: Speaker diarization
# ---------------------------------
t3 = time.time()
print("\nüß© Running speaker diarization...")

pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=HF_TOKEN
)

# Use MPS if available on Mac
if torch.backends.mps.is_available():
    pipeline.to(torch.device("mps"))
    print("üöÄ Using Apple Metal (MPS) for diarization")

diarization = pipeline(AUDIO_FILE)
print(f"‚úÖ Diarization complete in {time.time() - t3:.2f}s")

# ---------------------------------
# STEP 5: Assign speakers to segments
# ---------------------------------
t4 = time.time()
print("\nüß† Assigning speakers to segments...")

# Build speaker timeline for fast lookup
# speaker_timeline = []
# for turn, _, speaker in diarization.itertracks(yield_label=True):
#     speaker_timeline.append((turn.start, turn.end, speaker))

final_output = []
for seg in segments_list:
    start, end = seg["start"], seg["end"]
    # Find which speaker is active during this time
    active_speakers = [turn for turn in diarization.itertracks(yield_label=True)
                       if turn[0].start <= start <= turn[0].end]
    speaker = active_speakers[0][2] if active_speakers else "Unknown"
    final_output.append({"speaker": speaker, "text": seg["text"]})


print(f"‚úÖ Speaker alignment done in {time.time() - t4:.2f}s")

# ---------------------------------
# STEP 6: Save transcript with timestamps
# ---------------------------------
t5 = time.time()
print("\nüíæ Saving transcript...")

with open(f"transcription_with_speakers_{MODEL_SIZE}.json", "w") as f:
    json.dump(final_output, f, indent=2)

print(json.dumps(final_output[:5], indent=2))
print(f"‚úÖ Transcript saved in {time.time() - t5:.2f}s")

# ---------------------------------
# STEP 7: Summary
# ---------------------------------
total_time = time.time() - start_all
audio_duration = segments_list[-1]["end"] if segments_list else 0

print("\nüìä -------- PERFORMANCE SUMMARY --------")
print(f"Audio duration:       {format_time(audio_duration)}")
print(f"Conversion time:      {time.time() - t0:.2f}s")
print(f"Model loading:        {time.time() - t1:.2f}s")
print(f"Transcription:        {time.time() - t2:.2f}s")
print(f"Diarization:          {time.time() - t3:.2f}s")
print(f"Speaker alignment:    {time.time() - t4:.2f}s")
print(f"Saving:               {time.time() - t5:.2f}s")
print("----------------------------------------")
print(f"üïí TOTAL TIME: {total_time:.2f}s")
if audio_duration > 0:
    print(f"‚ö° Speed factor: {audio_duration/total_time:.2f}x realtime")
print("----------------------------------------")
print(f"‚úÖ Complete! Transcript saved to '{OUTPUT_FILE}'")